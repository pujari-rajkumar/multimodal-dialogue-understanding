{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0116fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 12:07:47.742090: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-05 12:07:47.773423: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-05 12:07:47.774150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-05 12:07:49.961855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import random\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import copy\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d44dd0-b41c-4235-92cc-952619072fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b92f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2c797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(13)\n",
    "torch.manual_seed(13)\n",
    "np.random.seed(13)\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e13029a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69141571",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/homes/rpujari/scratch_ml/DARPA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09e4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plutchik =  ['anger', 'fear', 'sadness', 'disgust', 'surprise', 'anticipation', 'trust', 'joy', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102b47f",
   "metadata": {},
   "source": [
    "### Loding LDC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb10083-ad68-4824-8728-5fe322771992",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldc_dpath = '/homes/rpujari/scratch0_ml/ldc_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a1468e-f779-4bdb-8236-1f2d35709feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ldc_emotion_data(data_version):\n",
    "    tfnames = os.listdir(ldc_dpath + data_version + '/source_data/text/txt/')\n",
    "    tfnames = [fn[:-4] for fn in tfnames]\n",
    "\n",
    "    #read and save all segments from segments.tab \n",
    "    #(all_segments: file_ID -> segment_name -> seg_bounds, rev_segments: -> file_ID -> seg_bounds -> segment_name)\n",
    "    as_c = 0\n",
    "    all_segments = {}\n",
    "    rev_segments = {}\n",
    "    seg_data = list(csv.reader(open(ldc_dpath + data_version + '/docs/segments.tab'), delimiter='\\t'))\n",
    "    #drop header\n",
    "    for row in seg_data[1:]:\n",
    "        #each row have file_ID, segment_name, start_char/start_secs, end_char/end_secs\n",
    "        ann_fname, seg_name, sc, ec = row\n",
    "        seg_bounds = (float(sc), float(ec))\n",
    "        if ann_fname in tfnames:\n",
    "            if ann_fname not in all_segments:\n",
    "                all_segments[ann_fname] = {}\n",
    "            all_segments[ann_fname][seg_name] = seg_bounds\n",
    "            \n",
    "            if ann_fname not in rev_segments:\n",
    "                rev_segments[ann_fname] = {}\n",
    "            rev_segments[ann_fname][seg_bounds] = seg_name\n",
    "\n",
    "    for tfname in all_segments:\n",
    "        as_c += len(all_segments[tfname])\n",
    "\n",
    "    #read and save annotations from perfect_submission for ED (annotations: file_ID -> seg_bounds -> emotion)\n",
    "    an_c = 0\n",
    "    annotations = {}\n",
    "    label_dist = {}\n",
    "    for tfname in tfnames:\n",
    "        if tfname not in annotations:\n",
    "            annotations[tfname] = {}\n",
    "        tf_anns = list(csv.reader(open(ldc_dpath + data_version + '/perfect_submissions/ED/' + tfname + '.tab'), delimiter='\\t'))\n",
    "        #drop header\n",
    "        for ann in tf_anns[1:]:\n",
    "            #each row has file_ID, annotated_emotion, start_char/start_secs, end_char/end_secs, llr (place_holder)\n",
    "            ann_fname, em, sc, ec, llr = ann\n",
    "            seg_bounds = (float(sc), float(ec))\n",
    "            if seg_bounds not in annotations[tfname]:\n",
    "                annotations[tfname][seg_bounds] = []\n",
    "            annotations[tfname][seg_bounds].append(em)\n",
    "            if em not in label_dist:\n",
    "                label_dist[em] = 0\n",
    "            label_dist[em] += 1\n",
    "        an_c += len(annotations[tfname])\n",
    "        \n",
    "    #randomly sample neutral segments\n",
    "    num_neutral_samples = int(np.mean([v for k, v in label_dist.items()]))\n",
    "    neutral_segments = []\n",
    "    for tfname in rev_segments:\n",
    "        for seg_bounds in rev_segments[tfname]:\n",
    "            if seg_bounds not in annotations[tfname]:\n",
    "                neutral_segments.append((tfname, seg_bounds))\n",
    "    sel_neutral_segments = random.sample(neutral_segments, min(num_neutral_samples, len(neutral_segments)))\n",
    "    label_dist['neutral'] = len(sel_neutral_segments)\n",
    "    \n",
    "    #add sampled segments to annotations\n",
    "    for tfname, seg_bounds in sel_neutral_segments:\n",
    "        if tfname not in annotations:\n",
    "            annotations[tfname] = {}\n",
    "        annotations[tfname][seg_bounds] = ['neutral']\n",
    "\n",
    "    return all_segments, rev_segments, annotations, label_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f116a9-9d19-4a14-8c5e-1e047c70bd61",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LDC Loading to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450983dd-9de9-4451-a326-f3696159034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldc_train = get_ldc_emotion_data('v5.1')\n",
    "ldc_test = get_ldc_emotion_data('unsequestered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8f2150-dd48-4ae2-b598-7bf964f9de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29477\n"
     ]
    }
   ],
   "source": [
    "ldc_intents_p1 = open('/homes/rpujari/scratch_ml/shared_models/gpt-neox/ldc_prompts/output_files/ldc_intent_outputs_p1.txt', 'r').readlines()\n",
    "ldc_intents_p2 = open('/homes/rpujari/scratch_ml/shared_models/gpt-neox/ldc_prompts/output_files/ldc_intent_outputs_p2.txt', 'r').readlines()\n",
    "ldc_intents = ldc_intents_p1 + ldc_intents_p2\n",
    "ldc_intents = [i for i in ldc_intents if i.strip()]\n",
    "\n",
    "intent_dict = {}\n",
    "for idx, intent in enumerate(ldc_intents):\n",
    "    d = json.loads(intent)\n",
    "    id_ = d['context'].strip().split('\\t')[0].split()[1]\n",
    "    i = d['text'].strip().split('\\t')[0].split('   ')[0].strip().split('\\n')[0].strip()\n",
    "    intent_dict[id_] = i\n",
    "    \n",
    "print(len(intent_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d301f815-5dd5-4959-be8b-4d2479b14dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 2176\n",
      "{'anger': 268, 'joy': 622, 'anticipation': 185, 'sadness': 216, 'disgust': 303, 'fear': 118, 'surprise': 255, 'trust': 23, 'neutral': 248}\n",
      "22 622\n",
      "{'fear': 38, 'disgust': 114, 'trust': 41, 'joy': 137, 'anticipation': 65, 'surprise': 81, 'sadness': 54, 'anger': 43, 'neutral': 71}\n",
      "1682 434 600\n"
     ]
    }
   ],
   "source": [
    "ldc_dset = {}\n",
    "ldc_dset[\"train\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "ldc_dset[\"valid\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "ldc_dset[\"test\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "\n",
    "context_len = 4\n",
    "\n",
    "segs, rev_segs, anns, label_dist = ldc_train\n",
    "multi_labels = 0\n",
    "tot = 0\n",
    "tlen = len(anns)\n",
    "fcount = 0\n",
    "t1 = datetime.now()\n",
    "for tfname in anns:\n",
    "    all_bounds = list(anns[tfname].keys())\n",
    "    all_bounds = sorted(all_bounds, key=lambda x:int(x[0]))\n",
    "    ftext = open(ldc_dpath + 'v5.1/source_data/text/txt/' + tfname + '.txt').read()\n",
    "    all_turns = []\n",
    "    for i, seg_bounds in enumerate(all_bounds):\n",
    "        if True: #seg_bounds in rev_segs[tfname]:\n",
    "            b, e = seg_bounds\n",
    "            b = int(b)\n",
    "            e = int(e)\n",
    "            # seg_name = rev_segs[tfname][seg_bounds]\n",
    "            all_turns.append(ftext[b:e+1])\n",
    "            toss = random.random()\n",
    "            if toss <= 0.8:\n",
    "                if len(anns[tfname][seg_bounds]) > 1:\n",
    "                    multi_labels += 1\n",
    "                else:\n",
    "                    ldc_dset[\"train\"][\"text\"].append(ftext[b:e+1])\n",
    "                    ldc_dset[\"train\"][\"intent\"].append(ftext[b:e+1])\n",
    "                    #ldc_dset[\"train\"][\"intent\"].append(intent_dict[seg_name])\n",
    "                    ldc_dset[\"train\"][\"context\"].append('\\n'.join(all_turns[max(0, i-context_len):i]))\n",
    "                    ldc_dset[\"train\"][\"label\"].append(plutchik.index(anns[tfname][seg_bounds][0]))\n",
    "            else:\n",
    "                if len(anns[tfname][seg_bounds]) > 1:\n",
    "                    multi_labels += 1\n",
    "                else:\n",
    "                    ldc_dset[\"valid\"][\"text\"].append(ftext[b:e+1])\n",
    "                    ldc_dset[\"valid\"][\"intent\"].append(ftext[b:e+1])\n",
    "                    #ldc_dset[\"valid\"][\"intent\"].append(intent_dict[seg_name])\n",
    "                    ldc_dset[\"valid\"][\"context\"].append('\\n'.join(all_turns[max(0, i-context_len):i]))\n",
    "                    ldc_dset[\"valid\"][\"label\"].append(plutchik.index(anns[tfname][seg_bounds][0]))\n",
    "    tot += len(anns[tfname])\n",
    "    fcount += 1\n",
    "    t2 = datetime.now()\n",
    "print(multi_labels, tot)\n",
    "print(label_dist)\n",
    "\n",
    "segs, rev_segs, anns, label_dist = ldc_test\n",
    "multi_labels = 0\n",
    "tot = 0\n",
    "for tfname in anns:\n",
    "    all_bounds = list(anns[tfname].keys())\n",
    "    all_bounds = sorted(all_bounds, key=lambda x:int(x[0]))\n",
    "    ftext = open(ldc_dpath + 'unsequestered/source_data/text/txt/' + tfname + '.txt').read()\n",
    "    all_turns = []\n",
    "    for i, seg_bounds in enumerate(all_bounds):\n",
    "        if True: #seg_bounds in rev_segs[tfname]:\n",
    "            b, e = seg_bounds\n",
    "            b = int(b)\n",
    "            e = int(e)\n",
    "            # seg_name = rev_segs[tfname][seg_bounds]\n",
    "            all_turns.append(ftext[b:e+1])\n",
    "            if len(anns[tfname][seg_bounds]) > 1:\n",
    "                multi_labels += 1\n",
    "            else:\n",
    "                ldc_dset[\"test\"][\"text\"].append(ftext[b:e+1])\n",
    "                ldc_dset[\"test\"][\"intent\"].append(ftext[b:e+1])\n",
    "                #ldc_dset[\"test\"][\"intent\"].append(intent_dict[seg_name])\n",
    "                ldc_dset[\"test\"][\"context\"].append('\\n'.join(all_turns[max(0, i-context_len):i]))\n",
    "                ldc_dset[\"test\"][\"label\"].append(plutchik.index(anns[tfname][seg_bounds][0]))\n",
    "    tot += len(anns[tfname])\n",
    "print(multi_labels, tot)\n",
    "print(label_dist)\n",
    "\n",
    "print(len(ldc_dset['train']['text']), len(ldc_dset['valid']['text']), len(ldc_dset['test']['text'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5288f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading MPDD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e325748",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_dialogues = json.load(open(fpath + 'mpdd/dialogue.json'))\n",
    "mpdd_metada = json.load(open(fpath + 'mpdd/metadata.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72cad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    'fear': 'fear',\n",
    "    'angry': 'anger',\n",
    "    'disgust': 'disgust',\n",
    "    'sadness': 'sadness',\n",
    "    'happiness': 'joy',\n",
    "    'surprise': 'surprise',\n",
    "    'neutral': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0401282",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_dset = {}\n",
    "mpdd_dset[\"train\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "mpdd_dset[\"valid\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "mpdd_dset[\"test\"] = {'text': [], 'context': [], 'intent': [], 'label': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d6abad-9a64-4ffb-b6ee-5537d1807948",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_intent = {}\n",
    "with open('/homes/rpujari/scratch_ml/shared_models/gpt-neox/mpdd_prompts/output_files/mpdd_intent_outputs.txt', 'r') as infile:\n",
    "    fc = infile.readlines()\n",
    "    for line in fc:\n",
    "        d = json.loads(line.strip())\n",
    "        id_ = d['context'].split('\\t')[0].split()[1].strip()\n",
    "        intent = d['text'].split('\\t')[0].split('    ')[0].strip().split('\\n')[0]\n",
    "        mpdd_intent[id_] = intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0410601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in mpdd_dialogues:\n",
    "    dialogue = mpdd_dialogues[id_]\n",
    "    context = ''\n",
    "    for i, turn in enumerate(dialogue):\n",
    "        toss = random.random()\n",
    "        if toss <= 0.7:\n",
    "            mpdd_dset[\"train\"]['text'].append(turn['utterance'])\n",
    "            mpdd_dset[\"train\"]['label'].append(plutchik.index(emotion_map[turn['emotion']]))\n",
    "            mpdd_dset[\"train\"]['intent'].append(mpdd_intent[id_ + '-' + str(i)])\n",
    "            mpdd_dset[\"train\"]['context'].append(context.strip())\n",
    "        elif toss <= 0.9:\n",
    "            mpdd_dset[\"valid\"]['text'].append(turn['utterance'])\n",
    "            mpdd_dset[\"valid\"]['label'].append(plutchik.index(emotion_map[turn['emotion']]))\n",
    "            mpdd_dset[\"valid\"]['intent'].append(mpdd_intent[id_ + '-' + str(i)])\n",
    "            mpdd_dset[\"valid\"]['context'].append(context.strip())\n",
    "        else:\n",
    "            mpdd_dset[\"test\"]['text'].append(turn['utterance'])\n",
    "            mpdd_dset[\"test\"]['label'].append(plutchik.index(emotion_map[turn['emotion']]))\n",
    "            mpdd_dset[\"test\"]['intent'].append(mpdd_intent[id_ + '-' + str(i)])\n",
    "            mpdd_dset[\"test\"]['context'].append(context.strip())\n",
    "        context += turn['speaker'] + '(' + emotion_map[turn['emotion']] + '): ' + turn['utterance'] + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10b118",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading CPED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90761d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cped_dset = {}\n",
    "cped_dset[\"train\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "cped_dset[\"valid\"] = {'text': [], 'context': [], 'intent': [], 'label': []}\n",
    "cped_dset[\"test\"] = {'text': [], 'context': [], 'intent': [], 'label': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f36fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plutchik = ['anger', 'fear', 'sadness', 'disgust', 'surprise', 'anticipation', 'trust', 'joy', 'neutral']\n",
    "\n",
    "# cped_emotion_map = {\n",
    "#     'grateful': 'trust',\n",
    "#     'neutral': 'neutral',\n",
    "#     'relaxed': 'joy',\n",
    "#     'positive-other': 'joy',\n",
    "#     'negative-other': 'sadness',\n",
    "#     'astonished': 'surprise',\n",
    "#     'sadness': 'sadness',\n",
    "#     'fear': 'fear',\n",
    "#     'worried': 'anticipation',\n",
    "#     'anger': 'anger',\n",
    "#     'depress': 'sadness',\n",
    "#     'disgust': 'disgust',\n",
    "#     'happy': 'joy'}\n",
    "\n",
    "cped_emotion_map = {\n",
    "    'grateful': 'trust',\n",
    "    'neutral': 'neutral',\n",
    "    'astonished': 'surprise',\n",
    "    'sadness': 'sadness',\n",
    "    'fear': 'fear',\n",
    "    'worried': 'anticipation',\n",
    "    'anger': 'anger',\n",
    "    'depress': 'sadness',\n",
    "    'disgust': 'disgust',\n",
    "    'happy': 'joy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4782b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffTV_ID', 'Dialogue_ID', 'Utterance_ID', 'Speaker', 'Gender', 'Age', 'Neuroticism', 'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Scene', 'FacePosition_LU', 'FacePosition_RD', 'Sentiment', 'Emotion', 'DA', 'Utterance']\n"
     ]
    }
   ],
   "source": [
    "fp = open(fpath + '/CPED/data/CPED/train_split.csv')\n",
    "fr = list(csv.reader(fp))\n",
    "i = 0\n",
    "header = None\n",
    "train_data = {}\n",
    "for row in fr:\n",
    "    if i > 0:\n",
    "        tv_id = row[0]\n",
    "        d_id = row[header.index('Dialogue_ID')]\n",
    "        ut_id = row[header.index('Utterance_ID')]\n",
    "        speaker = row[header.index('Speaker')]\n",
    "        ut = row[header.index('Utterance')]\n",
    "        em = row[header.index('Emotion')]\n",
    "        if em in cped_emotion_map:\n",
    "            if tv_id not in train_data:\n",
    "                train_data[tv_id] = {}\n",
    "            if d_id not in train_data[tv_id]:\n",
    "                train_data[tv_id][d_id] = {}\n",
    "            train_data[tv_id][d_id][ut_id] = (speaker, ut, em)                                                 \n",
    "    else:\n",
    "        header = row\n",
    "        print(row)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15dfe559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffTV_ID', 'Dialogue_ID', 'Utterance_ID', 'Speaker', 'Gender', 'Age', 'Neuroticism', 'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Scene', 'FacePosition_LU', 'FacePosition_RD', 'Sentiment', 'Emotion', 'DA', 'Utterance']\n"
     ]
    }
   ],
   "source": [
    "fp = open(fpath + '/CPED/data/CPED/valid_split.csv')\n",
    "fr = list(csv.reader(fp))\n",
    "i = 0\n",
    "header = None\n",
    "valid_data = {}\n",
    "for row in fr:\n",
    "    if i > 0:\n",
    "        tv_id = row[0]\n",
    "        d_id = row[header.index('Dialogue_ID')]\n",
    "        ut_id = row[header.index('Utterance_ID')]\n",
    "        speaker = row[header.index('Speaker')]\n",
    "        ut = row[header.index('Utterance')]\n",
    "        em = row[header.index('Emotion')]\n",
    "        if em in cped_emotion_map:\n",
    "            if tv_id not in valid_data:\n",
    "                valid_data[tv_id] = {}\n",
    "            if d_id not in valid_data[tv_id]:\n",
    "                valid_data[tv_id][d_id] = {}\n",
    "            valid_data[tv_id][d_id][ut_id] = (speaker, ut, em)                                                  \n",
    "    else:\n",
    "        header = row\n",
    "        print(row)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d095dd6-6713-4cff-80c7-5056542eea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffTV_ID', 'Dialogue_ID', 'Utterance_ID', 'Speaker', 'Gender', 'Age', 'Neuroticism', 'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Scene', 'FacePosition_LU', 'FacePosition_RD', 'Sentiment', 'Emotion', 'DA', 'Utterance']\n"
     ]
    }
   ],
   "source": [
    "fp = open(fpath + '/CPED/data/CPED/test_split.csv')\n",
    "fr = list(csv.reader(fp))\n",
    "i = 0\n",
    "header = None\n",
    "test_data = {}\n",
    "for row in fr:\n",
    "    if i > 0:\n",
    "        tv_id = row[0]\n",
    "        d_id = row[header.index('Dialogue_ID')]\n",
    "        ut_id = row[header.index('Utterance_ID')]\n",
    "        speaker = row[header.index('Speaker')]\n",
    "        ut = row[header.index('Utterance')]\n",
    "        em = row[header.index('Emotion')]\n",
    "        if em in cped_emotion_map:\n",
    "            if tv_id not in test_data:\n",
    "                test_data[tv_id] = {}\n",
    "            if d_id not in test_data[tv_id]:\n",
    "                test_data[tv_id][d_id] = {}\n",
    "            test_data[tv_id][d_id][ut_id] = (speaker, ut, em)                                                  \n",
    "    else:\n",
    "        header = row\n",
    "        print(row)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe90c01-7c86-4391-b7ee-0a8095f9d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132770 132763\n"
     ]
    }
   ],
   "source": [
    "cped_intent_path = '/homes/rpujari/scratch_ml/shared_models/gpt-neox/cped_prompts/outputs_intent/'\n",
    "cped_intents = {}\n",
    "cped_ifnames = os.listdir(cped_intent_path)\n",
    "t = 0\n",
    "for ifname in cped_ifnames:\n",
    "    ilines = open(cped_intent_path + ifname, 'r').readlines()\n",
    "    for iline in ilines:\n",
    "        if iline.strip():\n",
    "            d = json.loads(iline.strip())\n",
    "            try:\n",
    "                id_ = d['context'].strip().split('\\t')[0].split()[1].strip()\n",
    "                intent = d['text'].split('\\t')[0].split('    ')[0].strip().split('\\n')[0]\n",
    "                cped_intents[id_] = intent\n",
    "            except:\n",
    "                pass\n",
    "            t += 1\n",
    "print(t, len(cped_intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f71034f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'irony', 'greeting', 'comfort', 'thanking', 'statement-opinion', 'answer', 'acknowledge', 'agreement', 'command', 'interjection', 'question', 'apology', 'statement-non-opinion', 'disagreement', 'appreciation', 'conventional-closing', 'other', 'reject', 'quotation'}\n"
     ]
    }
   ],
   "source": [
    "das = set()\n",
    "for row in fr[1:]:\n",
    "    das.add(row[-2])\n",
    "print(das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f5335ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72850 72850\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for tv_id in train_data:\n",
    "    for d_id in train_data[tv_id]:\n",
    "        ut_ids = sorted(list(train_data[tv_id][d_id].keys()))\n",
    "        context = []\n",
    "        for i, ut_id in enumerate(ut_ids):\n",
    "            turn = train_data[tv_id][d_id][ut_id]\n",
    "            if ut_id in cped_intents and turn[2] in cped_emotion_map:    \n",
    "                cped_dset['train']['text'].append(turn[1])\n",
    "                cped_dset['train']['intent'].append(cped_intents[ut_id])\n",
    "                cped_dset['train']['context'].append('\\n'.join(context[-4:]))\n",
    "                cped_dset['train']['label'].append(plutchik.index(cped_emotion_map[turn[2]]))\n",
    "            context.append(turn[0] + '(' + turn[2] + '): ' + turn[1])\n",
    "            t += 1\n",
    "print(len(cped_dset['train']['text']), t)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f297c707-a407-4f2e-a0e6-641c10ff9bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8126 8126\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for tv_id in valid_data:\n",
    "    for d_id in valid_data[tv_id]:\n",
    "        ut_ids = sorted(list(valid_data[tv_id][d_id].keys()))\n",
    "        context = []\n",
    "        for i, ut_id in enumerate(ut_ids):\n",
    "            turn = valid_data[tv_id][d_id][ut_id]\n",
    "            if ut_id in cped_intents and turn[2] in cped_emotion_map:    \n",
    "                cped_dset['valid']['text'].append(turn[1])\n",
    "                cped_dset['valid']['intent'].append(cped_intents[ut_id])\n",
    "                cped_dset['valid']['context'].append('\\n'.join(context[-4:]))\n",
    "                cped_dset['valid']['label'].append(plutchik.index(cped_emotion_map[turn[2]]))\n",
    "            context.append(turn[0] + '(' + turn[2] + '): ' + turn[1])\n",
    "            t += 1\n",
    "print(len(cped_dset['valid']['text']), t)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ac794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21208 21208\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for tv_id in test_data:\n",
    "    for d_id in test_data[tv_id]:\n",
    "        ut_ids = sorted(list(test_data[tv_id][d_id].keys()))\n",
    "        context = []\n",
    "        for i, ut_id in enumerate(ut_ids):\n",
    "            turn = test_data[tv_id][d_id][ut_id]\n",
    "            if ut_id in cped_intents and turn[2] in cped_emotion_map:    \n",
    "                cped_dset['test']['text'].append(turn[1])\n",
    "                cped_dset['test']['intent'].append(cped_intents[ut_id])\n",
    "                cped_dset['test']['context'].append('\\n'.join(context[-4:]))\n",
    "                cped_dset['test']['label'].append(plutchik.index(cped_emotion_map[turn[2]]))\n",
    "            context.append(turn[0] + '(' + turn[2] + '): ' + turn[1])\n",
    "            t += 1\n",
    "print(len(cped_dset['test']['text']), t)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "018c898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72850 8126 21208\n"
     ]
    }
   ],
   "source": [
    "print(len(cped_dset['train']['text']), len(cped_dset['valid']['text']), len(cped_dset['test']['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96dd969",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading data to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c0a363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_train = Dataset.from_dict(mpdd_dset['train'])\n",
    "mpdd_valid = Dataset.from_dict(mpdd_dset['valid'])\n",
    "mpdd_test = Dataset.from_dict(mpdd_dset['test'])\n",
    "\n",
    "cped_train = Dataset.from_dict(cped_dset['train'])\n",
    "cped_valid = Dataset.from_dict(cped_dset['valid'])\n",
    "cped_test = Dataset.from_dict(cped_dset['test'])\n",
    "\n",
    "ldc_train = Dataset.from_dict(ldc_dset['train'])\n",
    "ldc_valid = Dataset.from_dict(ldc_dset['valid'])\n",
    "ldc_test = Dataset.from_dict(ldc_dset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00c2aa2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac0711d2a7341e79bdaa67e17347679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a6eb3cde0844408f13abd0eef72d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8443fd8ea4144e7e839fbe1152b24f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139586d135dd4317b940613aa1cde609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aefe54bd5434c2c8b62ae9eeffd5821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ab05133d634860911da4f3185385af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9be4b0c85848d99c049406fd1f0484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e28122bbd7b45c9a84aa142e600025d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ead24027f6461cafa1c547344038b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e4e9e1ae542e2be225450f9d49225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a0bc6635574661b40f953e4e5b2d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4750a5a56af947fcbc1c7b472d942f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd8797cdec046698977834f1610eee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc57018130d64f8d986c680bc4b4ebd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a47def5b6d54e8da99dfdcc776eba3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\",\\\n",
    "                     truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_mpdd_train = mpdd_train.map(tokenize_function, batched=True)\n",
    "tokenized_mpdd_valid = mpdd_valid.map(tokenize_function, batched=True)\n",
    "tokenized_mpdd_test = mpdd_test.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_cped_train = cped_train.map(tokenize_function, batched=True)\n",
    "tokenized_cped_valid = cped_valid.map(tokenize_function, batched=True)\n",
    "tokenized_cped_test = cped_test.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_ldc_train = ldc_train.map(tokenize_function, batched=True)\n",
    "tokenized_ldc_valid = ldc_valid.map(tokenize_function, batched=True)\n",
    "tokenized_ldc_test = ldc_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c610858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_mpdd_train = tokenized_mpdd_train.remove_columns([\"text\"])\n",
    "tokenized_mpdd_train = tokenized_mpdd_train.remove_columns([\"context\"])\n",
    "tokenized_mpdd_train = tokenized_mpdd_train.remove_columns([\"intent\"])\n",
    "tokenized_mpdd_train = tokenized_mpdd_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_mpdd_train.set_format(\"torch\")\n",
    "\n",
    "tokenized_mpdd_valid = tokenized_mpdd_valid.remove_columns([\"text\"])\n",
    "tokenized_mpdd_valid = tokenized_mpdd_valid.remove_columns([\"context\"])\n",
    "tokenized_mpdd_valid = tokenized_mpdd_valid.remove_columns([\"intent\"])\n",
    "tokenized_mpdd_valid = tokenized_mpdd_valid.rename_column(\"label\", \"labels\")\n",
    "tokenized_mpdd_valid.set_format(\"torch\")\n",
    "\n",
    "tokenized_mpdd_test = tokenized_mpdd_test.remove_columns([\"text\"])\n",
    "tokenized_mpdd_test = tokenized_mpdd_test.remove_columns([\"context\"])\n",
    "tokenized_mpdd_test = tokenized_mpdd_test.remove_columns([\"intent\"])\n",
    "tokenized_mpdd_test = tokenized_mpdd_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_mpdd_test.set_format(\"torch\")\n",
    "\n",
    "tokenized_cped_train = tokenized_cped_train.remove_columns([\"text\"])\n",
    "tokenized_cped_train = tokenized_cped_train.remove_columns([\"context\"])\n",
    "tokenized_cped_train = tokenized_cped_train.remove_columns([\"intent\"])\n",
    "tokenized_cped_train = tokenized_cped_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_cped_train.set_format(\"torch\")\n",
    "\n",
    "tokenized_cped_valid = tokenized_cped_valid.remove_columns([\"text\"])\n",
    "tokenized_cped_valid = tokenized_cped_valid.remove_columns([\"context\"])\n",
    "tokenized_cped_valid = tokenized_cped_valid.remove_columns([\"intent\"])\n",
    "tokenized_cped_valid = tokenized_cped_valid.rename_column(\"label\", \"labels\")\n",
    "tokenized_cped_valid.set_format(\"torch\")\n",
    "\n",
    "tokenized_cped_test = tokenized_cped_test.remove_columns([\"text\"])\n",
    "tokenized_cped_test = tokenized_cped_test.remove_columns([\"context\"])\n",
    "tokenized_cped_test = tokenized_cped_test.remove_columns([\"intent\"])\n",
    "tokenized_cped_test = tokenized_cped_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_cped_test.set_format(\"torch\")\n",
    "\n",
    "tokenized_ldc_train = tokenized_ldc_train.remove_columns([\"text\"])\n",
    "tokenized_ldc_train = tokenized_ldc_train.remove_columns([\"context\"])\n",
    "tokenized_ldc_train = tokenized_ldc_train.remove_columns([\"intent\"])\n",
    "tokenized_ldc_train = tokenized_ldc_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_ldc_train.set_format(\"torch\")\n",
    "\n",
    "tokenized_ldc_valid = tokenized_ldc_valid.remove_columns([\"text\"])\n",
    "tokenized_ldc_valid = tokenized_ldc_valid.remove_columns([\"context\"])\n",
    "tokenized_ldc_valid = tokenized_ldc_valid.remove_columns([\"intent\"])\n",
    "tokenized_ldc_valid = tokenized_ldc_valid.rename_column(\"label\", \"labels\")\n",
    "tokenized_ldc_valid.set_format(\"torch\")\n",
    "\n",
    "tokenized_ldc_test = tokenized_ldc_test.remove_columns([\"text\"])\n",
    "tokenized_ldc_test = tokenized_ldc_test.remove_columns([\"context\"])\n",
    "tokenized_ldc_test = tokenized_ldc_test.remove_columns([\"intent\"])\n",
    "tokenized_ldc_test = tokenized_ldc_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_ldc_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0ff724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_train_dataset = tokenized_mpdd_train.shuffle(seed=13).select(range(5000))\n",
    "mpdd_dev_dataset = tokenized_mpdd_valid.shuffle(seed=13).select(range(300))\n",
    "mpdd_test_dataset = tokenized_mpdd_test\n",
    "\n",
    "cped_train_dataset = tokenized_cped_train.shuffle(seed=13).select(range(5000))\n",
    "cped_dev_dataset = tokenized_cped_valid.shuffle(seed=13).select(range(300))\n",
    "cped_test_dataset = tokenized_cped_test\n",
    "\n",
    "ldc_train_dataset = tokenized_ldc_train\n",
    "ldc_dev_dataset = tokenized_ldc_valid\n",
    "ldc_test_dataset = tokenized_ldc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc5823af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdd_train_dataloader = DataLoader(mpdd_train_dataset, shuffle=True, batch_size=8)\n",
    "mpdd_dev_dataloader = DataLoader(mpdd_dev_dataset, batch_size=8)\n",
    "mpdd_test_dataloader = DataLoader(mpdd_test_dataset, batch_size=8)\n",
    "\n",
    "cped_train_dataloader = DataLoader(cped_train_dataset, shuffle=True, batch_size=8)\n",
    "cped_dev_dataloader = DataLoader(cped_dev_dataset, batch_size=8)\n",
    "cped_test_dataloader = DataLoader(cped_test_dataset, batch_size=8)\n",
    "\n",
    "ldc_train_dataloader = DataLoader(ldc_train_dataset, shuffle=True, batch_size=8)\n",
    "ldc_dev_dataloader = DataLoader(ldc_dev_dataset, batch_size=8)\n",
    "ldc_test_dataloader = DataLoader(ldc_test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81706f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f31c8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e194de-44d3-42fd-bfd9-8090818012f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_emotion_detector(model, lr, train_dataset, train_dataloader, dev_dataloader, data_tag, num_epochs=5, save_path=save_path):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    best_dev_perf = -1\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=(1.0 / (1 + train_dataset[\"labels\"].bincount())).to(device))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        bnum = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            bnum += 1\n",
    "            if bnum % 30 == 0:\n",
    "                model.eval()\n",
    "                metric = load(\"accuracy\")\n",
    "                for dev_batch in dev_dataloader:\n",
    "                    dev_batch = {k: v.to(device) for k, v in dev_batch.items()}\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**dev_batch)\n",
    "\n",
    "                    logits = outputs.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    metric.add_batch(predictions=predictions, references=dev_batch[\"labels\"])\n",
    "                perf = metric.compute()[\"accuracy\"]\n",
    "                print('Dev Accuracy: ', perf)\n",
    "                if perf > best_dev_perf:\n",
    "                    best_dev_perf = perf\n",
    "                    model.save_pretrained(save_path + 'emotion_detector_' + data_tag)\n",
    "                    print('Saving model parameters. Dev Accuracy: ', perf)\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "550a3d20-f819-4850-a949-8a357e9a7d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15423f4041e44b3b8c9efdfa6aabf4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-roberta-wwm-ext\", num_labels=9)\n",
    "model = model.to(device)\n",
    "\n",
    "# for param in model.base_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5474a8d-b6d0-4dd7-a39b-bc6940c72790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68ac6b75844a379385df89b759b938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(2.2723, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4077, -0.4569,  0.4603,  0.4018,  0.1258,  0.0121,  0.0657,  0.5181,\n",
      "         -0.0581],\n",
      "        [-0.3557, -0.5392,  0.4419,  0.7706, -0.0335, -0.2437,  0.0247,  0.3945,\n",
      "         -0.2664],\n",
      "        [ 0.0958, -0.2526,  0.2757,  0.3855,  0.1569,  0.0443,  0.0351,  0.4226,\n",
      "         -0.0539],\n",
      "        [-0.0991, -0.2664,  0.5650,  0.2519,  0.1277, -0.1910,  0.3592,  0.6872,\n",
      "         -0.1336],\n",
      "        [-0.0346, -0.1258,  0.2635,  0.2501, -0.2344, -0.0460,  0.3277,  0.6777,\n",
      "         -0.0840],\n",
      "        [-0.4546, -0.2947,  0.0289, -0.0836, -0.0565, -0.0246, -0.0879,  0.3951,\n",
      "          0.0313],\n",
      "        [-0.0199, -0.4560,  0.1270, -0.1186, -0.2350, -0.2780,  0.3204,  0.4294,\n",
      "         -0.1853],\n",
      "        [-0.2098, -0.4391,  0.3435,  0.3893, -0.3568,  0.1692,  0.3937,  0.4986,\n",
      "          0.1233]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "train_dataset = cped_train_dataset\n",
    "train_dataloader = cped_train_dataloader\n",
    "dev_dataloader = cped_dev_dataloader\n",
    "data_tag = 'cped'\n",
    "num_epochs=5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "best_dev_perf = -1\n",
    "loss_fn = nn.CrossEntropyLoss(weight=(1.0 / (1 + train_dataset[\"labels\"].bincount())).to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bnum = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        print(outputs)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff93b61-6cea-4941-8fcd-4a2d6ce9adfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_emotion_detector(model, 1e-5, cped_train_dataset, cped_train_dataloader, cped_dev_dataloader, 'cped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452a446-35e2-4da8-bbeb-af5c3e5d6b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_cped\", num_labels=9)\n",
    "model = model.to(device)\n",
    "train_emotion_detector(model, 1e-5, mpdd_train_dataset, mpdd_train_dataloader, mpdd_dev_dataloader, 'cped_mpdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea018de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_cped_mpdd\", num_labels=9)\n",
    "model = model.to(device)\n",
    "train_emotion_detector(model, 1e-5, ldc_train_dataset, ldc_train_dataloader, ldc_dev_dataloader, 'cped_mpdd_ldc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd451b-2c4b-4ff3-bcb6-09f567971713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_cped_mpdd_ldc\", num_labels=9)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097c55a-1359-4aa1-886b-182eb71ca897",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "model.eval()\n",
    "preds = []\n",
    "golds = []\n",
    "for batch in ldc_train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    preds.append(predictions)\n",
    "    golds.append(batch[\"labels\"])\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "metric.compute()\n",
    "\n",
    "pred = torch.cat(preds, dim=0).cpu().data.numpy()\n",
    "gold = torch.cat(golds, dim=0).cpu().data.numpy()\n",
    "\n",
    "print(classification_report(gold, pred, labels = list(range(9)), target_names=plutchik, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da85f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "model.eval()\n",
    "preds = []\n",
    "golds = []\n",
    "for batch in ldc_dev_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    preds.append(predictions)\n",
    "    golds.append(batch[\"labels\"])\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "metric.compute()\n",
    "\n",
    "pred = torch.cat(preds, dim=0).cpu().data.numpy()\n",
    "gold = torch.cat(golds, dim=0).cpu().data.numpy()\n",
    "\n",
    "print(classification_report(gold, pred, labels = list(range(9)), target_names=plutchik, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dd417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "model.eval()\n",
    "preds = []\n",
    "golds = []\n",
    "for batch in ldc_test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    preds.append(predictions)\n",
    "    golds.append(batch[\"labels\"])\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n",
    "\n",
    "pred = torch.cat(preds, dim=0).cpu().data.numpy()\n",
    "gold = torch.cat(golds, dim=0).cpu().data.numpy()\n",
    "\n",
    "print(classification_report(gold, pred, labels = list(range(9)), target_names=plutchik, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d11a9-6505-4af5-9acf-d5a1e7104f1e",
   "metadata": {},
   "source": [
    "### Eval Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685ef8a-92ed-43c2-81bd-57298b36bb05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Eval Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a382f-bfc6-44b9-8e9e-5e9ddde63769",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fol_names = os.listdir('/homes/rpujari/scratch0_ml/ldc_data/')\n",
    "eval_fol_names = [f for f in eval_fol_names if (f.startswith('eval-') and f != 'eval-zips') or '2022e22' in f]\n",
    "print(eval_fol_names)\n",
    "\n",
    "c = 0\n",
    "eval_data = {'id': [], 'text': [], 'label': []}\n",
    "eval_ids = {}\n",
    "idx = 0\n",
    "for fol_name in eval_fol_names:\n",
    "    tdata = {}\n",
    "    if os.path.exists(ldc_dpath + fol_name + '/data/text/'):\n",
    "        fnames = os.listdir(ldc_dpath + fol_name + '/data/text/ltf/')\n",
    "        for fname in fnames:\n",
    "            tdata[fname[:-8]] = {}\n",
    "            lsoup = BS(open(ldc_dpath + fol_name + '/data/text/ltf/' + fname).read(), 'xml')\n",
    "            ttags = lsoup.find_all('SEG')\n",
    "            c += len(ttags)\n",
    "            tups = []\n",
    "            for ttag in ttags:\n",
    "                ot = ttag.find('ORIGINAL_TEXT').text\n",
    "                sc = float(ttag.attrs['start_char'])\n",
    "                ec = float(ttag.attrs['end_char'])\n",
    "                tdata[fname[:-8]][(sc, ec)] = ot.strip()\n",
    "                eval_ids[idx] = (fol_name, fname[:-8], (sc, ec))\n",
    "                eval_data['id'].append(idx)\n",
    "                eval_data['text'].append(ot.strip())\n",
    "                eval_data['label'].append(0)\n",
    "                idx += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766d2d9-7f80-4166-8e01-3e9f0f24815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldc_eval = Dataset.from_dict(eval_data)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\",\\\n",
    "                     truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_ldc_eval = ldc_eval.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.remove_columns([\"text\"])\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.remove_columns([\"id\"])\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.rename_column(\"label\", \"labels\")\n",
    "tokenized_ldc_eval.set_format(\"torch\")\n",
    "\n",
    "ldc_eval_dataloader = DataLoader(tokenized_ldc_eval, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d3dd4-eea7-4bbb-9c06-ecb6f679222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "preds = []\n",
    "probs = []\n",
    "num_steps = len(ldc_eval_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_cped_mpdd_ldc\", num_labels=9)\n",
    "    model = model.to(device)\n",
    "    for batch in ldc_eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds.append(predictions)\n",
    "        probs.append(logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "ids = sorted(eval_ids.keys())\n",
    "\n",
    "predictions_out = torch.cat(preds, dim=0)\n",
    "print(predictions_out.size())\n",
    "predictions_prob = F.softmax(torch.cat(probs, dim=0), dim=1)\n",
    "print(predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed132fe-36e3-4528-9ed9-56e6e99a0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fws = {}\n",
    "out_cws = {}\n",
    "for id_ in ids:\n",
    "    if id_ < predictions_out.size(0):\n",
    "        fol_name, fname, (sc, ec) = eval_ids[id_]\n",
    "        if os.path.exists(ldc_dpath + fol_name + '/data/text/txt/' + fname + '.txt'):\n",
    "            if fname not in out_fws:\n",
    "                out_fw = open(ldc_dpath + 'outputs/text/ED/' + fname + '.tab', 'w')\n",
    "                out_fws[fname] = out_fw\n",
    "                out_cw = csv.writer(out_fw, delimiter='\\t')\n",
    "                out_cws[fname] = out_cw\n",
    "                out_cw.writerow(['file_id', 'emotion', 'start', 'end', 'llr'])\n",
    "            out_cw = out_cws[fname]\n",
    "            if plutchik[predictions_out[id_]] != 'neutral':\n",
    "                out_cw.writerow([fname, plutchik[predictions_out[id_]], sc, ec, float(torch.max(predictions_prob[id_, :]).data)])\n",
    "\n",
    "for fname in out_fws:\n",
    "    out_fws[fname].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc7072-8b57-49c4-b3cb-00a62751b6ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### NMAP data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67d828-53aa-4058-851f-415a149d1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fol_names = ['ldc2022e18_v6']\n",
    "print(eval_fol_names)\n",
    "\n",
    "c = 0\n",
    "eval_data = {'id': [], 'text': [], 'label': []}\n",
    "eval_ids = {}\n",
    "idx = 0\n",
    "for fol_name in eval_fol_names:\n",
    "    tdata = {}\n",
    "    if os.path.exists(ldc_dpath + fol_name + '/source_data/text/'):\n",
    "        fnames = os.listdir(ldc_dpath + fol_name + '/source_data/text/ltf/')\n",
    "        for fname in fnames:\n",
    "            tdata[fname[:-8]] = {}\n",
    "            lsoup = BS(open(ldc_dpath + fol_name + '/source_data/text/ltf/' + fname).read(), 'xml')\n",
    "            ttags = lsoup.find_all('SEG')\n",
    "            c += len(ttags)\n",
    "            tups = []\n",
    "            for ttag in ttags:\n",
    "                ot = ttag.find('ORIGINAL_TEXT').text\n",
    "                sc = float(ttag.attrs['start_char'])\n",
    "                ec = float(ttag.attrs['end_char'])\n",
    "                tdata[fname[:-8]][(sc, ec)] = ot.strip()\n",
    "                eval_ids[idx] = (fol_name, fname[:-8], (sc, ec))\n",
    "                eval_data['id'].append(idx)\n",
    "                eval_data['text'].append(ot.strip())\n",
    "                eval_data['label'].append(0)\n",
    "                idx += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1076d-a801-43eb-8265-0a76bb9b2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldc_eval = Dataset.from_dict(eval_data)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\",\\\n",
    "                     truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_ldc_eval = ldc_eval.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.remove_columns([\"text\"])\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.remove_columns([\"id\"])\n",
    "tokenized_ldc_eval = tokenized_ldc_eval.rename_column(\"label\", \"labels\")\n",
    "tokenized_ldc_eval.set_format(\"torch\")\n",
    "\n",
    "ldc_eval_dataloader = DataLoader(tokenized_ldc_eval, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a2a7c-14a9-4ed9-b4dc-3c41cead9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "preds = []\n",
    "probs = []\n",
    "num_steps = len(ldc_eval_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_cped_mpdd_ldc\", num_labels=9)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_ldc_gpt_17_c_test\", num_labels=17)\n",
    "    model = model.to(device)\n",
    "    for batch in ldc_eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds.append(predictions)\n",
    "        probs.append(logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "ids = sorted(eval_ids.keys())\n",
    "\n",
    "predictions_out = torch.cat(preds, dim=0)\n",
    "print(predictions_out.size())\n",
    "predictions_prob = F.softmax(torch.cat(probs, dim=0), dim=1)\n",
    "print(predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c5378-4136-4c27-9aef-6c5870600e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fws = {}\n",
    "out_cws = {}\n",
    "for id_ in ids:\n",
    "    if id_ < predictions_out.size(0):\n",
    "        fol_name, fname, (sc, ec) = eval_ids[id_]\n",
    "        if os.path.exists(ldc_dpath + fol_name + '/source_data/text/ltf/' + fname + '.ltf.xml'):\n",
    "            if fname not in out_fws:\n",
    "                out_fw = open(ldc_dpath + 'outputs_v6/text/ED/' + fname + '.tab', 'w')\n",
    "                out_fws[fname] = out_fw\n",
    "                out_cw = csv.writer(out_fw, delimiter='\\t')\n",
    "                out_cws[fname] = out_cw\n",
    "                out_cw.writerow(['file_id', 'emotion', 'start', 'end', 'llr'])\n",
    "            out_cw = out_cws[fname]\n",
    "            if plutchik[predictions_out[id_]] != 'neutral':\n",
    "                out_cw.writerow([fname, plutchik[predictions_out[id_]], sc, ec, float(torch.max(predictions_prob[id_, :]).data)])\n",
    "\n",
    "for fname in out_fws:\n",
    "    out_fws[fname].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c270260-4305-486f-808d-bc63a85453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "status_preds = []\n",
    "status_probs = []\n",
    "num_steps = len(ldc_eval_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    status_model = AutoModelForSequenceClassification.from_pretrained(save_path + \"norm_status_binary_ldc_text\", num_labels=2)\n",
    "    status_model = status_model.to(device)\n",
    "    for batch in ldc_eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            status_outputs = status_model(**batch)\n",
    "\n",
    "        status_logits = status_outputs.logits\n",
    "        status_predictions = torch.argmax(status_logits, dim=-1)\n",
    "        status_preds.append(status_predictions)\n",
    "        status_probs.append(status_logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "status_ids = sorted(eval_ids.keys())\n",
    "\n",
    "status_predictions_out = torch.cat(status_preds, dim=0)\n",
    "print(status_predictions_out.size())\n",
    "status_predictions_prob = F.softmax(torch.cat(status_probs, dim=0), dim=1)\n",
    "print(status_predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd550d4-a59b-46ca-8c45-e1f7b7ff2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_codes = [101, 102, 103, 105, 104, 107, 106, 201, 202, 203, 204, 205, 206, 1001, 207, 208, 209]\n",
    "status_codes = ['violate', 'adhere']\n",
    "\n",
    "out_fws = {}\n",
    "out_cws = {}\n",
    "for id_ in ids:\n",
    "    if id_ < predictions_out.size(0):\n",
    "        fol_name, fname, (sc, ec) = eval_ids[id_]\n",
    "        if os.path.exists(ldc_dpath + fol_name + '/source_data/text/ltf/' + fname + '.ltf.xml'):\n",
    "            if fname not in out_fws:\n",
    "                out_fw = open(ldc_dpath + 'outputs_v6/text/ND/' + fname + '.tab', 'w')\n",
    "                out_fws[fname] = out_fw\n",
    "                out_cw = csv.writer(out_fw, delimiter='\\t')\n",
    "                out_cws[fname] = out_cw\n",
    "                out_cw.writerow(['file_id', 'norm', 'start', 'end', 'status', 'llr'])\n",
    "            out_cw = out_cws[fname]\n",
    "            if norm_codes[predictions_out[id_]] != 1001:\n",
    "                stat = status_codes[status_predictions_out[id_]]\n",
    "                out_cw.writerow([fname, norm_codes[predictions_out[id_]], sc, ec, stat, float(torch.max(predictions_prob[id_, :]).data)])\n",
    "\n",
    "for fname in out_fws:\n",
    "    out_fws[fname].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f3ac0-fd8f-413d-ad61-af6a2835cede",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Norm Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7b354-ebe3-4a71-9e7b-24011d672398",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "preds = []\n",
    "probs = []\n",
    "num_steps = len(ldc_eval_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(save_path + \"emotion_detector_ldc_gpt_17_c_test\", num_labels=17)\n",
    "    model = model.to(device)\n",
    "    for batch in ldc_eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds.append(predictions)\n",
    "        probs.append(logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "ids = sorted(eval_ids.keys())\n",
    "\n",
    "predictions_out = torch.cat(preds, dim=0)\n",
    "print(predictions_out.size())\n",
    "predictions_prob = F.softmax(torch.cat(probs, dim=0), dim=1)\n",
    "print(predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e651d71-9d88-40f6-993b-0e2c931b78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_norm_categories = ['Doing apology', 'Doing criticism', 'Doing greeting', 'Doing persuasion',\\\n",
    "                   'Doing request', 'Doing taking leave', 'Doing thanks', 'acknowledging',\\\n",
    "                   'expressing opinion', 'giving explanation', 'making clarification', 'making invitation',\\\n",
    "                   'making suggestion', 'none', 'offering reassurance', 'questioning', 'responding to request']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01478b7-0b7c-4f3a-af5c-bd1fc38b63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "status_preds = []\n",
    "status_probs = []\n",
    "num_steps = len(ldc_eval_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    status_model = AutoModelForSequenceClassification.from_pretrained(save_path + \"norm_status_binary_ldc_text\", num_labels=2)\n",
    "    status_model = status_model.to(device)\n",
    "    for batch in ldc_eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            status_outputs = status_model(**batch)\n",
    "\n",
    "        status_logits = status_outputs.logits\n",
    "        status_predictions = torch.argmax(status_logits, dim=-1)\n",
    "        status_preds.append(status_predictions)\n",
    "        status_probs.append(status_logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "status_ids = sorted(eval_ids.keys())\n",
    "\n",
    "status_predictions_out = torch.cat(status_preds, dim=0)\n",
    "print(status_predictions_out.size())\n",
    "status_predictions_prob = F.softmax(torch.cat(status_probs, dim=0), dim=1)\n",
    "print(status_predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed86c7-5992-4465-93f1-e6a3416414f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_codes = [101, 102, 103, 105, 104, 107, 106, 201, 202, 203, 204, 205, 206, 1001, 207, 208, 209]\n",
    "status_codes = ['violate', 'adhere']\n",
    "\n",
    "out_fws = {}\n",
    "out_cws = {}\n",
    "for id_ in ids:\n",
    "    if id_ < predictions_out.size(0):\n",
    "        fol_name, fname, (sc, ec) = eval_ids[id_]\n",
    "        if os.path.exists(ldc_dpath + fol_name + '/data/text/txt/' + fname + '.txt'):\n",
    "            if fname not in out_fws:\n",
    "                out_fw = open(ldc_dpath + 'outputs/text/ND/' + fname + '.tab', 'w')\n",
    "                out_fws[fname] = out_fw\n",
    "                out_cw = csv.writer(out_fw, delimiter='\\t')\n",
    "                out_cws[fname] = out_cw\n",
    "                out_cw.writerow(['file_id', 'norm', 'start', 'end', 'status', 'llr'])\n",
    "            out_cw = out_cws[fname]\n",
    "            if norm_codes[predictions_out[id_]] != 1001:\n",
    "                stat = status_codes[status_predictions_out[id_]]\n",
    "                out_cw.writerow([fname, norm_codes[predictions_out[id_]], sc, ec, stat, float(torch.max(predictions_prob[id_, :]).data)])\n",
    "\n",
    "for fname in out_fws:\n",
    "    out_fws[fname].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce40ba-9c85-4806-8734-e75e0223d0c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ChangePoint Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6db79-a361-4c74-870b-f39170ac6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fol_names = os.listdir('/homes/rpujari/scratch0_ml/ldc_data/')\n",
    "eval_fol_names = [f for f in eval_fol_names if (f.startswith('eval-') and f != 'eval-zips') or '2022e22' in f]\n",
    "print(eval_fol_names)\n",
    "\n",
    "c = 0\n",
    "idx = 0\n",
    "eval_cp_data = {}\n",
    "for fol_name in eval_fol_names:\n",
    "    tdata = {}\n",
    "    if os.path.exists(ldc_dpath + fol_name + '/data/text/'):\n",
    "        fnames = os.listdir(ldc_dpath + fol_name + '/data/text/ltf/')\n",
    "        for fname in fnames:\n",
    "            tdata[fname[:-8]] = {}\n",
    "            lsoup = BS(open(ldc_dpath + fol_name + '/data/text/ltf/' + fname).read(), 'xml')\n",
    "            ttags = lsoup.find_all('SEG')\n",
    "            c += len(ttags)\n",
    "            for ttag in ttags:\n",
    "                ot = ttag.find('ORIGINAL_TEXT').text\n",
    "                sc = float(ttag.attrs['start_char'])\n",
    "                ec = float(ttag.attrs['end_char'])\n",
    "                tdata[fname[:-8]][(sc, ec)] = (idx, ot.strip())\n",
    "                idx += 1\n",
    "    eval_cp_data[fol_name] = tdata\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a5985-2800-4939-b287-cf5f0ce5521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = {'id': [], 'text_a': [], 'text_b': []}\n",
    "cp_ids = {}\n",
    "\n",
    "dp_num = 0\n",
    "for fol_name in eval_cp_data:\n",
    "    for fname in eval_cp_data[fol_name]:\n",
    "        seg_bos = list(eval_cp_data[fol_name][fname].keys())\n",
    "        seg_bos = sorted(seg_bos, key=lambda x:int(x[0]))\n",
    "        l = len(seg_bos)\n",
    "        if l > 10:\n",
    "            st = 4\n",
    "            en = l - 5\n",
    "            for idx in range(st, en):\n",
    "                bs_ = ' '.join([eval_cp_data[fol_name][fname][bo][1] for bo in seg_bos[idx-4:idx]])[-50:]\n",
    "                as_ = ' '.join([eval_cp_data[fol_name][fname][bo][1] for bo in seg_bos[idx:idx+4]])[:50]\n",
    "                cp_data['id'].append(dp_num)\n",
    "                cp_data['text_b'].append(bs_)\n",
    "                cp_data['text_a'].append(as_)\n",
    "                cp_ids[dp_num] = (fol_name, fname, idx)\n",
    "                dp_num += 1\n",
    "print(dp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633b655-a581-4ff5-b3d0-e644aa3ae6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cp_dset = Dataset.from_dict(cp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b4637-681c-4a59-9bde-e1d91c2bf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text_b'], examples['text_a'], padding=\"max_length\",\\\n",
    "                     truncation=True, max_length=512)\n",
    "\n",
    "tokenized_eval_cp = eval_cp_dset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263cd78b-58a8-442f-bf19-4bc8ed413019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eval_cp = tokenized_eval_cp.remove_columns([\"text_a\"])\n",
    "tokenized_eval_cp = tokenized_eval_cp.remove_columns([\"text_b\"])\n",
    "tokenized_eval_cp = tokenized_eval_cp.remove_columns([\"id\"])\n",
    "tokenized_eval_cp.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb886225-4cf1-43ba-9b3b-35051bd10bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cp_dataloader = DataLoader(tokenized_eval_cp, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12934d-4ba9-4b18-8f06-e872a5022825",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/homes/rpujari/scratch_ml/DARPA/ta2snapshot_saved_parameters/'\n",
    "preds = []\n",
    "probs = []\n",
    "num_steps = len(eval_cp_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(save_path + \"change_point_ldc_text\", num_labels=2)\n",
    "    model = model.to(device)\n",
    "    for batch in eval_cp_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds.append(predictions)\n",
    "        probs.append(logits)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "ids = sorted(eval_ids.keys())\n",
    "\n",
    "predictions_out = torch.cat(preds, dim=0)\n",
    "print(predictions_out.size())\n",
    "predictions_prob = F.softmax(torch.cat(probs, dim=0), dim=1)\n",
    "print(predictions_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad91ef6-c959-434f-8085-cc2043a8b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sorted(cp_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cab930-dd54-4d78-aa89-5e0065c45a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fws = {}\n",
    "out_cws = {}\n",
    "p = 0\n",
    "n = 0\n",
    "counts = {}\n",
    "fname_cps = {}\n",
    "for id_ in ids:\n",
    "    if id_ < predictions_out.size(0):\n",
    "        fol_name, fname, idx = cp_ids[id_]\n",
    "        seg_bos = list(eval_cp_data[fol_name][fname].keys())\n",
    "        seg_bos = sorted(seg_bos, key=lambda x:int(x[0]))\n",
    "        ts = seg_bos[idx][0]\n",
    "        if fname not in counts:\n",
    "            counts[fname] = 0\n",
    "        if os.path.exists(ldc_dpath + fol_name + '/data/text/txt/' + fname + '.txt'):\n",
    "            if predictions_out[id_] == 1:\n",
    "                if fname not in fname_cps:\n",
    "                    fname_cps[fname] = []\n",
    "                counts[fname] += 1\n",
    "                fname_cps[fname].append((fname, ts, float(torch.max(predictions_prob[id_, :]).data)))\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "for fname in fname_cps:\n",
    "    cp_list = fname_cps[fname]\n",
    "    cp_list = sorted(cp_list, key=lambda x:x[2], reverse=True)\n",
    "    out_fw = open(ldc_dpath + 'outputs/text/CD/' + fname + '.tab', 'w')\n",
    "    out_cw = csv.writer(out_fw, delimiter='\\t')\n",
    "    out_cw.writerow(['file_id', 'timestamp', 'llr'])\n",
    "    for i in range(3):\n",
    "        if i < len(cp_list):\n",
    "            row = cp_list[i]\n",
    "            out_cw.writerow([row[0], row[1], row[2]])\n",
    "            p += 1\n",
    "    out_fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a88143-f3be-4fb6-90cd-4964c4db74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb78b2-b7f1-4607-87b1-31df6bdbaf94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
